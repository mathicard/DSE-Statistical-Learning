---
title: "Statistical Learning Project"
author: "Above the norm - Bassini, Cardarello, Ciarrocchi, Cutrera, Maroni, Rossi"
date: "5/21/2021"
abstract: "Environmental protection nowadays is a more and more important issue on which countries are investing many resources. What we are interested in is to find a way to classify a country in the United States as polluted or not on a bunch of explanatory variables we thought could be relevant. Therefore, we let the data tell us how we can explain pollution in United States. In this paper we try to find out a model with relevant variables that describe what affects pollution, moreover, we make some attempt in order to classify each single country as polluted or clean on a discrete scale. [...]"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
```

```{r include=FALSE}
data <- read.csv('usa_final.csv', sep=',')
data <- data[,-c(20,21)]
log_aqi <- log(data$aqi)
data$ln_aqi <- log_aqi
```

# Data and Problem Understanding
The starting point of our research question was the effect of the lockdown we experienced in 2020 on the environment. From media and social networks, we have always heard about the positive effects that the lockdowns spread all over the world had on our air quality. Then, we start considering the variable that explains the strictness of lockdown (lockdown yes = 1, lockdown no = 0, soft lockdown = 0.5) in relation with the air quality index. As you can notice from the graph below, the implementation of lockdown did not have any real effect on the air quality. Indeed, the distributions in the three boxplots are not statistically different: they have quite similar means and the distributions seem to be overlapping. 

```{r include=FALSE}
library(car)
library(readr)
library(dplyr)
library(tidyr)
library(readxl)
library(ggpubr)
library(ggplot2)
library(Hmisc)
library(olsrr)
library(tidyverse)
library(caret)
library(Metrics)
library(leaps)
```


```{r, fig.align="center", out.width="70%"}
# Visualize the expression profile
dataset <- read.csv("usa_final.csv", sep=',', header = TRUE, dec = ".")
dataset <- dataset %>% mutate(ln_aqi=log(dataset$aqi))
ggboxplot(dataset, x = "lockdown", y = "aqi", color = "lockdown", 
          add = "jitter", legend = "none") +
  geom_hline(yintercept = mean(dataset$aqi), linetype = 2)+ # Add horizontal line at base mean
  ylim(0, 200)+
  stat_compare_means(method = "anova", label.y = 200)+        # Add global ANOVA p-value
  stat_compare_means(label = "p.signif", method = "t.test",
                     ref.group = ".all.", hide.ns = TRUE)      # Pairwise comparison against all


# We can conclude that AQI is not significantly different between States by lockdown measures.
```

Since we got these unexpected results, we decided to go further with our analysis. 
Therefore, from now on we would like to find the real factors that explain the difference of the air quality among US countries in 2020. 
We started constructing our dataset scraping many sources on the web, such as Bureau of Economic Analysis, the Environmental protection agency of US, United States Census Bureau et cetera. 


The first approach we had with our data consisted in the simple full model of linear regression (OLS). Then, we computed the variance inflation factor (vif) for each variable and we found some 'dangerous' values: according to Hair et al., 1995 the maximum acceptable level of vif is 10, whereas according to Ringle et al., 2015 the maximum acceptable level of vif is 5. 
Starting with the approach of Hair et al. (threshold = 10), we exlcluded from our dataset the following regressors: waste and healthcare; thereafter, we performed also the vif according to Ringle et al., approach (threshold = 5) and we drop construction, utilities, professional, retail and finance from our analysis. 
Then, we computed the OLS with the remaining variables and we looked for normality on the residuals: they are normally distributed.
We went on performing a model selection in order to find the relevant regressors among the others.
Given that our number of predictors (17) is not higher than the number of observations (51) we could not perform the backward stepwise model selection; then we decided to implement the forward search in order to choose the "best" predictors for our dependent variable (air quality). 

For the classification task:
- at first we decided to adopt the Nereast Neighbor algorithm (K-NN) which seemed to perform pretty well (accuracy = 81.25%);
- then, we run the classification trees but it did not output results as good as the K-NN. Therefore, we tried to improve its performance implementing the random forest algorithm and we got an accuracy of 75% (but it is still not good as the one of K-NN)

To sum up our main results we can say that there exist at least one model which describes relevant factors for distinguishing countries with good air quality from those which are more polluted. The relevant factors we found are: the share of rural population out of the total, the manufactoring contribution to GDP, the annual rainfalls and the density of the factories for each country (nÂ° of factories / squared kilometres). 


## Data Description

We started our data scraping from many sources, and we found a dependent variable to describe air quality, and many other independent variables for each of the 51 States of the United States.

We list below the full dataset we built: 

- aqi: Air Quality Index
the Air Quality Index is a yardstick that runs from 0 to 500. The higher tha Air Quality Index value, the greater the level of air pollution^[Source: EPA - United States Environmental Protection Agency <https://aqs.epa.gov/aqsweb/airdata/download_files.html#Annual>]. 
We computed aqi as the average of the median value for each county of each state in 2020 and we found a minimum of 59.25 for Hawaii and a maximum of 197.81 for California. 
The Air Quality Index presented six possible discrete categories: 

  + Green (good) = from 0 to 50 

  + Yellow (moderate) = from 51 to 100 

  + Orange (unhealthy for sensitive groups) = from 101 to 150 

  + Red (unhealthy) = from 151 to 200 

  + Purple (very unhealthy) = from 201 to 300 

  + Maroon (hazardous) = 301 and higher. 

From this Air Quality Index we discretize the variable called 'polluted' that categorize our entities in three levels of pollution: Yellow, Orange and Red. (Since we only have States that range from 59.25 to 197.81).
 
- accommodation, construction, education, finance, healthcare, information, manufacturing, mining, professional, retail, transportation, utilities, waste: they are contributions to Gross Domestic Product by State^[Source: BEA - Bureau of Economic Analysis
<https://apps.bea.gov/itable/iTable.cfm?ReqID=70&step=1#reqid=70&step=1&isuri=1>] for the industry specified in 2020.
They are measured in millions of current dollars. 


- mining: it represents the value of nonfuel mineral production per square kilometer in dollars (2017)^[Source: USGS Minerals Yearbook 2018
<https://prd-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/atoms/files/myb1-2017-stati-adv.xlsx>]. 
(The District of Columbia has no mineral production). 


- precipitations: it is measured in inches and it specifies how much in a country has rained in a year (2020)^[Source: Statista
<https://www.statista.com/statistics/1101518/annual-precipitation-by-us-state/>]. 


- lockdown: it indicates if the state had a stay-at-home order (1), an advisory or a regional measure (0.5)* or nothing (0). (In the State of Wisconsin, the lockdown was declared unconstitutional after a month)^[Source: Wikipedia
<https://en.wikipedia.org/wiki/U.S._state_and_local_government_responses_to_the_COVID-19_pandemic#Initial_pandemic_responses,_including_full_lockdowns>]. 


- pop_rural: It is a decennial census of the population for each state and it distinguishes rural from urban population^[Source: United States Census Bureau
<https://www.census.gov/programs-surveys/geography/guidance/geo-areas/urban-rural/2010-urban-rural.html>].
We considered the percentage of the rural population out of the total as a measure of being a rural state. 


- n_factories: Number of manufacturing firms in each state in 2017^[Source: National Association of Manufacturers
<https://www.nam.org/state-manufacturing-data/>] divided by the surface of each country. Therefore, it represents the density of factories in each US state. 

The structure of our dataset, formed by 51 observations and 19 variables without missing values, is as follows:

```{r include=FALSE}
library(ggplot2)
library(dplyr)
library(RCurl)
x <- getURL("https://raw.githubusercontent.com/mathicard/Statistical-Learning-DSE/main/usa_final.csv")
dataset <- read.csv(text = x)
```

```{r, out.width="50%", fig.align="center"}
str(dataset[,1:19])
```


## Data Visualization and Exploration

We start the exploration analysis with our dependent variable *aqi* that describes the Air Quality of each of the 51 States of U.S. in 2020. The mean is 95.57 with a standar deviation of 25.76. The complete descriptive statistics are in the following table.

<br/>

```{r message=FALSE, out.width="50%", fig.align="center"}
dataset %>% summarise(
  count = n(), 
  mean = mean(aqi, na.rm = TRUE),
  median = median(aqi),
  min = min(aqi),
  max = max(aqi),
  sd = sd(aqi, na.rm = TRUE)
)
```

<br/>

From the boxplot we can easily visualize the distribution of the Air quality index, and especially the names of the States that were the most polluted in 2020. Therefore, we can consider California (197.81), District of Columbia (166) and Wyoming (148.22) as outliers in the dataset. 

<br/>

```{r, message=FALSE, out.width="60%", fig.align="center"}
Boxplot(~aqi, data=data, col="#69b3a2", id=list(labels=data$state))
```

<br/>

Then, we compare the distribution of the Air Quality Index within the three levels of the factor *polluted*. The share of entities that have a "yellow" level of pollution is more than 70%, while 26% are "orange" and the left are "red" (4%). As we can see in the boxplots, except for the only two States that have a "red" AQI, "orange" has the most disperse distribution with a standard deviation of 14.8. In fact, the two extreme values 101 and 148 are almost in the minimum and maximum boundaries of that level of pollution. Finally, the presence of two States with a high AQI could introduce some noise in our analysis that should be take it into account. 

<br/>

```{r message=FALSE, include=FALSE}

# AQI categorization
dataset$polluted <- cut(dataset$aqi, breaks = c(50,100,150,200),
               labels = c('yellow', 'orange', 'red'))

```


```{r message=FALSE, out.width="50%", fig.align="center"}

## Descriptive statistics by group ##

group_by(dataset, polluted) %>% 
  summarise(
    count = n(), 
    share = n()/51*100,
    mean = mean(aqi, na.rm = TRUE),
    median = median(aqi),
    min = min(aqi),
    max = max(aqi),
    sd = sd(aqi, na.rm = TRUE)
  )

#Box plot

ggboxplot(dataset, x = "polluted", y = "aqi", color = "polluted",
          palette = c("#E7B800", "#FC4E07", "red"), 
          add = "jitter", legend = "none") +
  geom_hline(yintercept = mean(dataset$aqi), linetype = 2)+ # Add horizontal line at base mean
  ylim(0, 200)
```

<br/>

Looking at the distribution plot it seems that AQI has a left-skewed Normal distribution. To be sure, we use the well-known Shapiro-Wilk test in order to test the null hypothesis of normality. Unfortunately it is rejected at 95% of confidence, so our variable needs to be transformed.

<br/>

```{r message=FALSE, out.width="50%", fig.align="center"}
dataset %>%
    ggplot(aes(x=aqi)) +
    geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8)+
    labs(title = 'Density Plot of Air Quality Index', subtitle='Levels')

shapiro.test(dataset$aqi)
```

<br/>

A possible way to improve our results could be by applying the logarithmic transformation. With the logarithm it could be seen that the skewness to the left is sharpened, and now as a matter of fact, Shapiro-Wilk test is not rejecting the normality hypothesis at 95% of confidence (p-value>0.05).

```{r, include=FALSE}
dataset <- dataset %>% mutate(ln_aqi=log(dataset$aqi))
```

```{r, out.width="80%", fig.align="center"}
shapiro.test(dataset$ln_aqi)
```

In the following density plots we can see the improvement after applying the logarithm function to our dependent variable, which now seems more similar to a Normal distribution. 

<br/>

```{r, out.width="80%", fig.align="center"}
d1 <- dataset %>%
  ggplot(aes(x=ln_aqi)) +
  geom_density(fill="orangered3", color=FALSE, alpha=0.5)+
  labs(title = 'Density Plot of Air Quality Index', subtitle='Logarithmic scale')
d2 <- dataset %>%
    ggplot(aes(x=aqi)) +
    geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8)+
    labs(title = 'Density Plot of Air Quality Index', subtitle='Levels')
ggarrange(d2, d1, 
          ncol = 2, nrow = 1)
```

<br/>

As almost all the points of our sample fall approximately along the reference line in the QQ-plot, we can assume normality of the AQI variable. This assumption allows us to work with statistical hypothesis tests that assume that the data follow a Normal distribution, and also to met the Central Limit Theorem assumptions and the normality of the residuals to build a regression model. 

<br/>


```{r, fig.align="center", out.width="70%"}
ggqqplot(dataset$ln_aqi, title = "QQ Plot of log AQI")
```

<br/>

In the following correlation matrix we analyze the relationship between the variables of our dataset, computing the correlation coefficients with their p-values as to check for the significance. 

<br/>

```{r, message=FALSE, out.width="60%", fig.align="center"}
library(corrplot)

data_corr <- dataset[,-c(1,20:23)]
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
M <- cor(data_corr)

cor.mtest <- function(mat, ...) {
  mat <- as.matrix(mat)
  n <- ncol(mat)
  p.mat<- matrix(NA, n, n)
  diag(p.mat) <- 0
  for (i in 1:(n - 1)) {
    for (j in (i + 1):n) {
      tmp <- cor.test(mat[, i], mat[, j], ...)
      p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
    }
  }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}

# matrix of the p-value of the correlation
p.mat <- cor.mtest(data_corr)
colnames(M) <- c("aqi", "accom", "const", "edu", "fin", "health", "info", 
                 "manuf", "mining", "prof", "retail", "trans", "util", "waste",
                 "precip", "lock", "p_rural", "n_fact")

corrplot(M, method="color", col=col(200),  
         type="upper", order="hclust", 
         addCoef.col = "black", 
         tl.col="black", tl.srt=45, 
         p.mat = p.mat, sig.level = 0.05, 
         number.cex = .6,
         diag=FALSE)
```

<br/>

Almost all variables have a positive correlation with AQI, except for pop_rural which has an expected negative sign due to the fact that the air quality improves when the share of rural population of the State is greater. While education, mining,  precipitations and lockdown have a non significative correlation with our variable of interest. Moreover, as it was expected, the variables related to the GDP of each State show a strong positive correlation between them indicating the presence of a possible multicollinearity problem. Finally, pop_rural has a negative correlation with a large number of variables of the dataset, including AQI.

<br/>

```{r, message=FALSE, out.width="60%", fig.align="center"}

# Perform the ANOVA test
compare_means(aqi ~ lockdown,  data = dataset,
              ref.group = ".all.", method = "t.test")

# Visualize the expression profile
ggboxplot(dataset, x = "lockdown", y = "aqi", color = "lockdown", 
          add = "jitter", legend = "none") +
  geom_hline(yintercept = mean(dataset$aqi), linetype = 2)+ # Add horizontal line at base mean
  ylim(0, 200)+
  stat_compare_means(method = "anova", label.y = 200)+        # Add global ANOVA p-value
  stat_compare_means(label = "p.signif", method = "t.test",
                     ref.group = ".all.", hide.ns = TRUE)      # Pairwise comparison against all

```

<br/>

As to check if the air quality differs among the intensity of lockdown measures applied by each State in 2020, we perform an ANOVA test. According to the results, where the t-statistic is lower than 2 with a p-value large than 0.05 (0.37), the *aqi* doesn't differ for *lockdown* at 95% of confidence. Therefore, we can conclude that AQI is not significantly different between States by lockdown measures.

<br/>





```{r, include=FALSE}
log_aqi <- log(data$aqi)
data$ln_aqi <- log_aqi
dt <- data %>% drop_na()
dt <- dt[,-c(1,2)]
```

\tiny
```{r, include=FALSE}
full.model <- lm(ln_aqi~.-waste-healthcare-construction-utilities-professional-retail-finance, data = dt)
summary(full.model)
```
\normalsize
# Data Analysis for Supervised Learning

## Model Selection - Forward stepwise

Our dataset, which is composed by 51 observations (the 51 countries of the United States of America) and 18 explanatory variables, does not need any other manipulation since we do not have any missing values. The only things we have done are: the transformation of the dependent variable (Air Quality) with the logarithmic scale, in order to get a "more" normal distribution as explained in the previous section, and the standarization of all our variables, which has given us the re-scaling with the same unit of measurement. 
Once our dataset was ready, we had an attempt with the simplest way we know to construct a model: the Ordinary least Square. In the full model the most significative variables seem to be the number of factories (with positive sign, as expected) and precipitations (with negative sign, as expected). 

```{r}
full.model <- lm(ln_aqi~., data = dt)
summary(full.model)
```

But before reaching conclusions, we have to control many things in our model of regression; one of the most important is the variance inflation factor (vif) which gives us insights into the variables which can be "dangerous" to what concern multicollinearity. 

```{r}
vif(full.model)
sqrt(vif(full.model))>10
```

In fact some explanatory variables have a very high variance inflation factor, and we decided to drop these ones taking as benchmark the thresholds cited in Hair et al., 1995 (in which the maximum acceptable level of vif is 10) before, and then also the more strict one according to Ringle et al., 2015 (in which it was 5). 
After that our model of linear regression still has the number of factories and precipitations as the most statistically significatives explanatory variables, with no more any predictor with high vif.

```{r}
full.model <- lm(ln_aqi~.-waste-healthcare-construction-utilities-professional-retail-finance, data = dt)
summary(full.model)
vif(full.model)
sqrt(vif(full.model))>5
```

Fitting our model of regression on a random training set, we can visualize that residuals are normally distributed:

```{r, include=FALSE}
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x))) 
}
dt <- as.data.frame(lapply(dt, normalize))

set.seed(123)
train = sample(1:nrow(dt), 0.7*nrow(dt))
dt_train = dt[train,-18]
dt_test = dt[-train,-18]
dt_train_labels <- dt[train, 18]
dt_test_labels <- dt[-train, 18]
```


```{r}
library(Metrics)
full.model <- lm(ln_aqi~.-waste-healthcare-construction-utilities-professional-retail-finance, data = dt[train,])
ols_plot_resid_fit(full.model)
ols_plot_resid_hist(full.model)
```

And making the predictions on our unaccessed test set, we can see the plot actual versus predicted values on the test set, to see whether they are far from being in the straight line of zero errors:

```{r}
pred_ols <- predict(full.model, dt[-train,])
ggplot(dt_test, aes(x=pred_ols, y=dt_test_labels)) + 
  geom_point() +
  geom_abline(intercept=0, slope=1) +
  labs(x='Predicted Values', y='Actual Values', title='Predicted vs. Actual Values')
```

## Model Selection - LASSO 
```{r}
library(glmnet)

x=model.matrix(ln_aqi~., dt)[,-1]
y=dt$ln_aqi

fit.lasso=glmnet(x,y)
plot(fit.lasso,xvar="lambda",label=TRUE)

cv.lasso=cv.glmnet(x,y)
plot(cv.lasso)

coef(cv.lasso)

```

we use our earlier train/validation division to select the lambda for the lasso.

```{r}
lasso.tr=glmnet(x[train,],y[train])
lasso.tr

pred=predict(lasso.tr,x[-train,])
dim(pred)

rmse= sqrt(apply((y[-train]-pred)^2,2,mean))
plot(log(lasso.tr$lambda),rmse,type="b",xlab="Log(lambda)")

lam.best=lasso.tr$lambda[order(rmse)[1]]
lam.best

coef(lasso.tr,s=lam.best)
```

Variabless with the highest relevance are manufacturing, precipitatons, pop_rural and n_factories

```{r, include=FALSE}
dt <- dt[,-c(13, 5, 2, 12, 9, 10, 4)]
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x))) 
}
#dt <- as.data.frame(lapply(dt, normalize))
set.seed(123)
train = sample(1:nrow(dt), 0.7*nrow(dt))
dt_train = dt[train,-11]
dt_test = dt[-train,-11]
dt_train_labels <- dt[train, 11]
dt_test_labels <- dt[-train, 11]
```

With the method of the forward search which starts from the null model, and step by step adds the most relevant variable (in the bunch of the remaining ones) included in the model with the lowest Residual Sum of Squares, or highest R^2. Plotting the cross-validated prediction error (with Bayes Information Criterion - BIC) we can find the parsimonious model which is performing best, because the bic takes also into account the number of predictors which are included in our models (as a penalization term for the models with more variables). 

```{r}
regfit.fwd=regsubsets(ln_aqi~.,data=dt,method="forward", nvmax=10)
summary(regfit.fwd)
```

```{r, message = FALSE}
reg.summary<-summary(regfit.fwd)
plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",types="l")
```

Now in our linear model made up by the 4 variables selected, the only one which becomes no more significative is the rural population indicator, but tHe signs are all as expected.

```{r}
model <- lm(ln_aqi~ pop_rural + manufacturing + precipitations + n_factories, 
            data = dt[train,])
summary(model)
```

```{r}
pred_fwd <- predict(model, dt[-train,])
root_mse = rmse(dt_test_labels, pred_fwd)
root_mse
```

At least our new model created with the forward search has a lower Mean Squared Error on the prediction made on the test set. Before it was 0.1823493, now it is 0.1317688.

## Tree classifiers 

After modelling, we would like to make an attempt and we try to classify our 51 countries according to the pollution of their air quality. 
Therefore, we decided to apply the tree algorithm for classification. 

We started including all the variables in our analysis since we want the tree to select the most important ones for classification. 
In the plot below there is a graphical representation of the procedure of how the tree split the data. 
This tree has a misclassification error rate of approximately 18%.

```{r, message = FALSE}
library(dplyr)
data$cl <- cut(data$aqi, breaks = c(50,100,150,200),
               labels = c('yellow', 'orange', 'red'))
dt <- dt[,-11]
dt$polluted <- data$cl
dt$polluted <- as.factor(dt$polluted)

#train test split

set.seed(123)
train = sample(1:nrow(dt), 0.7*nrow(dt))
dt_train = dt[train,-11]
dt_test = dt[-train,-11]
dt_train_labels <- dt[train, 11]
dt_test_labels <- dt[-train, 11]
summary(dt_train_labels)
summary(dt_test_labels)

###############################################################################
library(tree)
library(ISLR)

tree = tree(polluted~., dt)
summary(tree)
plot(tree)
text(tree, pretty = 0)
```

We tried to do the train and the test and we got a low accuracy of 62.5%. 

```{r, message = FALSE}
tree_train <- tree(polluted~., dt[train,])
tree_pred <- predict(tree_train, dt[-train,], type = 'class')
table(tree_pred, dt_test_labels)

accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(table(tree_pred, dt_test_labels))
```

We wanted to improve the performance of our classification task and we decided to reduce the dimensionality of our dataset selecting only the most relevant variables that were chosen in the model selection section.

Reducing the number of variables lead us to get a slightly lower misclassification error rate of 16%. 
Below you can find the plot of the tree. 
```{r, message = FALSE}
#TREE with relevany variables
tree = tree(polluted~pop_rural + manufacturing + precipitations 
            + n_factories, dt)
summary(tree)
plot(tree)
text(tree, pretty = 0)
```

We train our algorithm and we then test since we thought we have found better results. 
However, as you can notice we did not increase our accuracy that remains equal to 62.5%. 
```{r, message = FALSE}
tree_train <- tree(polluted~pop_rural + manufacturing + precipitations 
                   + n_factories, dt[train,])
tree_pred <- predict(tree_train, dt[-train,], type = 'class')
table(tree_pred, dt_test_labels)

accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(table(tree_pred, dt_test_labels))
```

We wanted to do one last attempt to improve the accuracy of our tree classifier and we applied the random forest. 

```{r, message = FALSE}
#random forest
library(randomForest)
rf.tree=randomForest(polluted~.,data=dt,subset=train)
rf.tree

pred_rf <- predict(rf.tree, dt_test)
table(pred_rf, dt_test_labels)
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(table(pred_rf, dt_test_labels))
```
As you can see, the predictions on the test set output an improved accuracy of 75%. 

Even though we applied the random forest and we got better results, we were not completely satisfied with the accuracy of this method of classification. Therefore, we decided to apply the K-Nearest Neighbours algorithm which will perform better and it will achieve a higher accuracy. 

## k-Nearest Neighbours

In this section we apply the K-Nearest Neighbor supervised algorithm (KNN) with the aim of predicting the air quality category to which each State belongs, according to the AQI 2020. 

In first place, we normalize the variables in our dataset so that distances between variables with larger ranges will not be over-emphasized. Therefore, the values of all the features are in the range of 0 and 1.

```{r, include=FALSE, warning = FALSE}
x <- getURL("https://raw.githubusercontent.com/mathicard/Statistical-Learning-DSE/main/usa_final.csv")
dataset <- read.csv(text = x)

#AQI categorization
dataset$polluted <- cut(dataset$aqi, breaks = c(50,100,150,200),
                        labels = c('yellow', 'orange', 'red'))

dt <- as.data.frame(cbind(lapply(dataset[,-c(1,2,20,21,22)], normalize), dataset[,c(1,20,21,22)]))
```

Then, we split the original labeled dataset into training (70%) and test data (30%), keeping a similar distribution of instances so that we do not favor one or the other class in the predictions.


```{r, include=FALSE, warning = FALSE}

set.seed(12)
library(caret)
train.index <- createDataPartition(dt$polluted, p = .7, list = FALSE)
dt_train <- dt[ train.index,-c(18,19,20)]
dt_test  <- dt[-train.index,-c(18,19,20)]

dt_train_labels <- dt[train.index, 21]
dt_test_labels <- dt[-train.index, 21]

table1 <- table(dt_train_labels)
round(prop.table(table1), 2)

table2 <- table(dt_test_labels)
round(prop.table(table2), 2)

dt_train <- dt_train[,-18]
dt_test <- dt_test[,-18]
```


In order to assess the classification accuracy, we plot the relationship between the training and test error error rates as a function of the number of neighbors selected (K). As K increases, the method becomes less flexible. The training error rate consistently increases as the flexibility decreases. Both error rates declines at first, reaching a minimum rate at K=5, before increasing again when the method becomes more inflexible. Therefore, we chose K=5 as the best number of neighbors for KNN in our prediction. 

```{r, warning = FALSE}
##load the package class
library(class)

### Plot training-test error by K value

library(class)
error.train <- replicate(0,20)

for(k in 1:20) {
  pred_pol <- knn(train = dt_train, test = dt_test, cl = dt_train_labels, k)
  error.train[k]<-1-mean(pred_pol==dt_train_labels)
}

error.train <- unlist(error.train, use.names=FALSE)

error.test <- replicate(0,20)
for(k in 1:20) {
  pred_pol <- knn(train = dt_train, test = dt_test, cl = dt_train_labels, k)
  error.test[k]<-1-mean(pred_pol==dt_test_labels)
}

error.test <- unlist(error.test, use.names = FALSE)

plot(error.train, type="o", col="blue", ylim=c(0,0.5), xlab = "K values", ylab = "Misclassification errors")
lines(error.test, type = "o", ylim=c(0,0.5), col="red")
legend("topright", legend=c("Training error","Test error"), col = c("blue","red"), lty=1:1)

#which(error.train==min(error.train)) # k=5
#which(error.test==min(error.test)) # k=5
```

By setting K=5, we get a model with an accuracy of 85% with the misclassification of only two "orange" States (Arizona and Michigan) which are categorized as "yellow".

```{r, warning = FALSE}

##run knn function (k=5)
pr <- knn(dt_train,dt_test,cl=dt_train_labels,k=5)

##create confusion matrix
tab <- table(pr,dt_test_labels)
tab

##this function divides the correct predictions by total number of predictions that tell us how accurate the model is.

accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tab)

#Accuracy of 85%
```

Moreover, we perform a repeated Cross Validation algorithm to choose K among the performed models according to its accuracy, our evaluation measure. In the next figure it is plotted the Number of Neighbours Vs Accuracy and we can see that by fixing K = 7 we get the model with the largest accuracy (72%).

```{r, warning = FALSE}
set.seed(12)
ctrl <- trainControl(method="repeatedcv",repeats = 3) #,classProbs=TRUE,summaryFunction = twoClassSummary)

knnFit <- train(polluted ~ .,
                      data = dt[ train.index,-c(18,19,20)], method = "knn", trControl = ctrl, 
                      preProcess = c("center","scale"), tuneLength = 20)

plot(knnFit)
#knnFit$bestTune #higher accuracy with k=7

#Output of kNN fit
knnFit
```

In spite of the presence of multicollinearity among our variables, as we saw in the previous section, the predictions should not been affected in the KNN modeling. Therefore, we decided to run this classification algorithm with only the four most relevant variables to predict AQI selected by the Forward Search that was already performed (*"manufacturing"*,*"pop_rural"*,*"precipitations"* and *"n_factories"*). However, the accuracy was a little bit improved, by choosing K=5 or K=9 the metric was almost 77% in both cases.


```{r, warning = FALSE}

library(class)
error.train <- replicate(0,20)

dt_train2 <- dt_train[,c("manufacturing","pop_rural","precipitations")]
dt_test2 <- dt_test[,c("manufacturing","pop_rural","precipitations")]


for(k in 1:20) {
  pred_pol <- knn(train = dt_train2, test = dt_test2, cl = dt_train_labels, k)
  error.train[k]<-1-mean(pred_pol==dt_train_labels)
}

error.train <- unlist(error.train, use.names=FALSE)

error.test <- replicate(0,20)
for(k in 1:20) {
  pred_pol <- knn(train = dt_train2, test = dt_test2, cl = dt_train_labels, k)
  error.test[k]<-1-mean(pred_pol==dt_test_labels)
}

error.test <- unlist(error.test, use.names = FALSE)

plot(error.train, type="o", col="blue", ylim=c(0,0.7), xlab = "K values", ylab = "Misclassification errors")
lines(error.test, type = "o", ylim=c(0,0.5), col="red")
legend("topright", legend=c("Training error","Test error"), col = c("blue","red"), lty=1:1)

#which(error.train==min(error.train)) # k>=5
#which(error.test==min(error.test)) # k=5


##run knn function (k=5)
pr5 <- knn(dt_train2,dt_test2,cl=dt_train_labels,k=5)

##create confusion matrix
tab <- table(pr5,dt_test_labels)
tab

accuracy(tab)

#Accuracy of 84.6%

combinetest <- cbind(dt_test, dt[-train.index, 18])
combinetest[dt_test_labels != pr5,]

#Michigan and Oklahoma are the States misclassified. Let see the AQI

dataset[dataset$state=="Oklahoma", 2] #102.1
dataset[dataset$state=="Michigan", 2] #114.11
```


Finally, we keep the three most important variables (*"manufacturing"*,*"pop_rural"*,*"precipitations"*) and with K= 5 the accuracy obtained was 85%, equal to the largest value we saw in the KNN model performed with all the variables. In this case, the only two States that were misclassified are Michigan (114.1) and Oklahoma (102.1), both belonging to the "orange" category and which the model categorized as "yellow". 


```{r, warning = FALSE}

#Plot in 3 dimensional space

#install.packages("plot3D")
library(plot3D)

colVar <- factor(dt$polluted)


scatter3D(dt$pop_rural, dt$manufacturing, dt$precipitations, 
          labels = rownames(dt),
          colvar=as.integer(colVar),
          phi = 0, bty ="g",
          pch = 20, cex = 1.5,
          col = c("gold2", "chocolate1", "red3"),
          xlab = "Rural pop",
          ylab ="Manufacturing", zlab = "Precipitations",
          colkey = list(at = c(1, 2, 3), side = 4, 
                        addlines = TRUE, length = 0.5, width = 0.5,
                        labels = c("yelow", "orange", "red")))

text3D(dt$pop_rural, dt$manufacturing, dt$precipitations,  
       labels = dt$Code,
       add = TRUE, colkey = FALSE, cex = 0.5)
```


In conclusion, we keep this last KNN model to predict the category at which State belongs according to its level of air quality, by only using the GDP of Manufacturing, the Share of Rural population and the Quantity of Precipitations.


# Data Analysis for Unsupervised Learning

# Theoretical background of the used methods (optional)
- some mathematical formula of what we used, not so difficult

# Conclusions 

The starting point of our research was to check if lockdowns (used as measures of containment of COVID-19) 
had influenced the quality of air in the United States in 2020, as often stated by media.

We started subdivising the states according to their air quality in three categories: yellow, orange and red (from least to most polluted)  
Proceeding with the analysis of the datasets through the boxplots and the ANOVA test, we see that there is no strong relation between lockdown and the AQI,
which presents a mean and a standard deviation normally distributed in almost all the states, with the exception of a few ouliers.
After this first analysis, there's evidence that the AQI doesn't differ significantly in states where there has been a lockdown.

Proceeding with the OLS on the best parsimonious model selected through the BIC, we found the most relevant variables,
which are: number of factoriesm precipitations, population in rural areas and the level of GDP in manufacture.
The first two present the most significant statistical relation with the AQI, with a positive and negative sign respectively of course.
Also the level of population in rural areas presents a negative sign, as expected.

Finally, we computed a KNN algorithm to predict the AQI, based on all the variables and the three categories aforementioned, obtaining an 85% accuracy with only two misclassification.
The subselection of variables most relevant to this model drops to three, the number of factories gets dropped, but the others are the same as above.

The conclusion is that in the US, contrarily from a lot of statements circulated through the media,
which suggested that lockdowns had a positive influence on the air quality, the relation between these two element is poor.
In fact, other indicator have much more influence on air quality, such as precipitations, people living in rural areas, 
number of factories (many of them still operating during the pandemic, especially if related to primary goods), all of which
couldn't be controlled by the imposition of a lockdown. 

# Appendix (optional)
- containing all the R code now visualized (put option echo=TRUE in r chunk)
